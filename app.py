import os
import json
import requests
import streamlit as st
from datetime import datetime
from dotenv import load_dotenv
from pytrends.request import TrendReq
from requests.utils import quote
from openai import OpenAI
import schedule
import time
import matplotlib.pyplot as plt
import numpy as np
import threading

# =============================
# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
# =============================
load_dotenv()
PIXABAY_API_KEY = os.getenv("TNPIXABAY")
UNSPLASH_API_KEY = os.getenv("TNUNSPLASH")
OPENAI_API_KEY = os.getenv("TNSYSTEM1")
client = OpenAI(api_key=OPENAI_API_KEY)

# =============================
# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
# =============================
PDCA_DIR = "pdca_reports"
SUMMARY_DIR = "summary_reports"
LEARNING_FILE = "learning_data.json"
os.makedirs(PDCA_DIR, exist_ok=True)
os.makedirs(SUMMARY_DIR, exist_ok=True)
if not os.path.exists(LEARNING_FILE):
    with open(LEARNING_FILE, "w", encoding="utf-8") as f:
        json.dump([], f, indent=2)

# =============================
# å¸‚å ´èª¿æŸ»å¯¾è±¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
# =============================
TARGET_KEYWORDS = ["fitness", "eco", "travel"]

# =============================
# APIãƒ‡ãƒ¼ã‚¿å–å¾—
# =============================
def get_pixabay_count(keyword: str) -> int:
    url = f"https://pixabay.com/api/?key={PIXABAY_API_KEY}&q={quote(keyword)}&image_type=photo"
    res = requests.get(url, timeout=10)
    if res.status_code == 200:
        return res.json().get("totalHits", 0)
    return 0

def get_unsplash_count(keyword: str) -> int:
    url = f"https://api.unsplash.com/search/photos?query={quote(keyword)}"
    headers = {"Authorization": f"Client-ID {UNSPLASH_API_KEY}"}
    res = requests.get(url, headers=headers, timeout=10)
    if res.status_code == 200:
        return res.json().get("total", 0)
    return 0

def get_trends_score(keyword: str) -> int:
    pytrends = TrendReq(hl='en-US', tz=360)
    pytrends.build_payload([keyword], cat=0, timeframe='today 12-m', geo='', gprop='')
    data = pytrends.interest_over_time()
    if not data.empty:
        return int(data[keyword].mean())
    return 50

# =============================
# å¸‚å ´èª¿æŸ»å®Ÿè¡Œ
# =============================
def run_market_research(keywords=TARGET_KEYWORDS):
    results = []
    for kw in keywords:
        pixabay_hits = get_pixabay_count(kw)
        unsplash_hits = get_unsplash_count(kw)
        trends_score = get_trends_score(kw)
        competition_score = min(100, (pixabay_hits + unsplash_hits) // 1000)
        final_score = max(0, min(100, trends_score - competition_score/2))
        results.append({
            "keyword": kw,
            "pixabay_hits": pixabay_hits,
            "unsplash_hits": unsplash_hits,
            "trends_score": trends_score,
            "competition_score": competition_score,
            "final_score": final_score
        })
    return results

# =============================
# PDCAç”Ÿæˆ
# =============================
def generate_pdca_report(results):
    with open(LEARNING_FILE, "r", encoding="utf-8") as f:
        learning_data = json.load(f)
    recent_history = learning_data[-5:] if len(learning_data) >= 5 else learning_data

    prompt = f"""
ã‚ãªãŸã¯å¸‚å ´èª¿æŸ»AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
éå»ã®å­¦ç¿’å±¥æ­´:
{recent_history}

æœ€æ–°èª¿æŸ»çµæœ:
{results}

ã“ã®æƒ…å ±ã‹ã‚‰PDCAã‚µã‚¤ã‚¯ãƒ«ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
å‡ºåŠ›å½¢å¼:
åŸå› :
çµæœ:
è€ƒå¯Ÿ:
æ”¹å–„æ¡ˆ:
å…·ä½“çš„ãªè¡Œå‹•è¨ˆç”»:
"""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        max_tokens=500,
        messages=[
            {"role": "system", "content": "ã‚ãªãŸã¯å†·é™ã§è«–ç†çš„ãªå¸‚å ´åˆ†æã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content

# =============================
# å­¦ç¿’å±¥æ­´ä¿å­˜
# =============================
def save_learning_entry(user_input, bot_response):
    with open(LEARNING_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)
    data.append({
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "user_input": user_input,
        "bot_response": bot_response
    })
    with open(LEARNING_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)

# =============================
# ãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ä¿å­˜
# =============================
def daily_task():
    results = run_market_research()
    pdca_text = generate_pdca_report(results)
    filename = f"{PDCA_DIR}/report_{datetime.now().strftime('%Y%m%d')}.md"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(f"# æ—¥æ¬¡å¸‚å ´èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(str(results)+"\n\n")
        f.write(pdca_text)
    print(f"âœ… æ—¥æ¬¡ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {filename}")

def summarize_reports(period="monthly"):
    files = sorted(os.listdir(PDCA_DIR))
    if not files:
        print("âš ï¸ ã¾ã¨ã‚ã‚‹ãƒ¬ãƒãƒ¼ãƒˆãªã—")
        return
    target_files = files[-30:]  # éå»30æ—¥åˆ†
    content = ""
    for f in target_files:
        with open(os.path.join(PDCA_DIR, f), "r", encoding="utf-8") as file:
            content += f"\n\n=== {f} ===\n\n" + file.read()
    prompt = f"ç›´è¿‘ã®{period}ãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿ã€å‚¾å‘ãƒ»æˆæœãƒ»èª²é¡Œãƒ»æ”¹å–„æ¡ˆãƒ»è¡Œå‹•è¨ˆç”»ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚\n{content}"
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        max_tokens=800,
        messages=[
            {"role": "system", "content": "ã‚ãªãŸã¯å†·é™ã§è«–ç†çš„ãªå¸‚å ´åˆ†æã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
            {"role": "user", "content": prompt}
        ]
    )
    summary_text = response.choices[0].message.content
    summary_filename = f"{SUMMARY_DIR}/{period}_{datetime.now().strftime('%Y%m%d')}.md"
    os.makedirs(SUMMARY_DIR, exist_ok=True)
    with open(summary_filename, "w", encoding="utf-8") as f:
        f.write(summary_text)
    print(f"âœ… {period}ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {summary_filename}")

# =============================
# ã‚°ãƒ©ãƒ•åŒ–
# =============================
def plot_scores(results):
    labels = [r["keyword"] for r in results]
    trends = [r["trends_score"] for r in results]
    competition = [r["competition_score"] for r in results]
    final = [r["final_score"] for r in results]
    x = np.arange(len(labels))
    plt.figure(figsize=(10,5))
    plt.plot(x, trends, marker='o', label='Trends Score')
    plt.plot(x, competition, marker='o', label='Competition Score')
    plt.plot(x, final, marker='o', label='Final Score')
    plt.xticks(x, labels)
    plt.ylim(0, 100)
    plt.title("å¸‚å ´èª¿æŸ»ã‚¹ã‚³ã‚¢")
    plt.xlabel("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
    plt.ylabel("ã‚¹ã‚³ã‚¢")
    plt.legend()
    plt.grid(True)
    st.pyplot(plt)

# =============================
# ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
# =============================
def run_scheduler():
    # æ¯é€±é‡‘æ›œæ—¥ 0:00 ã®ã¿
    schedule.every().friday.at("00:00").do(daily_task)
    schedule.every(30).days.at("23:55").do(lambda: summarize_reports("monthly"))
    while True:
        schedule.run_pending()
        time.sleep(60)

# ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©èµ·å‹•
threading.Thread(target=run_scheduler, daemon=True).start()

# =============================
# Streamlit UI
# =============================
st.set_page_config(page_title="ğŸ“Š å¸‚å ´èª¿æŸ»AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ", page_icon="ğŸ“ˆ")
st.title("ğŸ“Š å¸‚å ´èª¿æŸ» AI ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")

user_input = st.text_input("æŒ‡ç¤ºã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if user_input:
    st.session_state.chat_history.append({"role": "user", "content": user_input})

    # ä¼šè©±ç”Ÿæˆ
    with open(LEARNING_FILE, "r", encoding="utf-8") as f:
        recent_learning = json.load(f)[-5:]

    conversation_context = st.session_state.chat_history + recent_learning
    prompt_messages = [{"role": "system", "content": "ã‚ãªãŸã¯å¸‚å ´èª¿æŸ»ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"}]
    for m in conversation_context:
        content = m.get("content") if m.get("content") is not None else ""
        role = "user" if m.get("role")=="user" else "assistant"
        prompt_messages.append({"role": role, "content": content})

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        max_tokens=500,
        messages=prompt_messages
    )
    bot_output = response.choices[0].message.content
    st.session_state.chat_history.append({"role": "assistant", "content": bot_output})
    save_learning_entry(user_input, bot_output)

# ä¼šè©±è¡¨ç¤º
for msg in st.session_state.chat_history:
    if msg["role"] == "user":
        st.markdown(f"**ã‚ãªãŸ:** {msg['content']}")
    else:
        st.markdown(f"**ãƒœãƒƒãƒˆ:** {msg['content']}")

# æ‰‹å‹•ã§å¸‚å ´èª¿æŸ» & ã‚°ãƒ©ãƒ•è¡¨ç¤º
if st.button("ğŸ”¹ æ‰‹å‹•ã§å¸‚å ´èª¿æŸ»å®Ÿè¡Œ"):
    manual_results = run_market_research()
    st.write(manual_results)
    plot_scores(manual_results)
    pdca_text = generate_pdca_report(manual_results)
    st.text_area("PDCAåˆ†æ", pdca_text, height=300)
